{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sb\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import networkx as nx\n",
    "from mlxtend.frequent_patterns import apriori, association_rules, fpgrowth\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from statsmodels.discrete.discrete_model import Logit\n",
    "from statsmodels.iolib.summary2 import summary_col\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error , r2_score\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('cylinder4.xlsx')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rules\n",
    "df['rules'] = ((df['BCT_Avg'] < df['BCT_Spec']) & ((((df['BCT_Avg'] - df['BCT_Spec'])/df['BCT_Spec'])*100) < 0.03)).astype(int)\n",
    "df['Finished_Product_Hig'] = (((300<df['Finished_Product_Hig']) & (df['Finished_Product_Hig']<600))).astype(int)\n",
    "df['Speed_Act_bin'] =(df['Speed_Act']<350).astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO remove all useless columns \n",
    "try:\n",
    "    df.drop('ï»¿Ã¯Â»Â¿Index',inplace= True , axis =1)\n",
    "    df.drop('Order_Number',inplace= True , axis =1)\n",
    "\n",
    "    df.drop('Raw_Mat_Combination',inplace= True , axis =1)\n",
    "    df.drop('GL',inplace=True,axis=1)\n",
    "    df.drop('CL',inplace=True,axis=1)\n",
    "    df.drop('New_Printing_Gap_1',inplace=True,axis=1)\n",
    "    df.drop('New_Printing_Gap_2',inplace=True,axis=1)\n",
    "    df.drop('New_Printing_Gap_3',inplace=True,axis=1)\n",
    "    df.drop('New_Printing_Gap_4',inplace=True,axis=1)\n",
    "    df.drop('New_Average_Printing_Gap',inplace=True,axis=1)\n",
    "    df.drop('Number_of_use_Printing_Cylinder',inplace=True,axis=1)\n",
    "    df.drop('Rwf',inplace=True,axis=1)\n",
    "    df.columns\n",
    "except:\n",
    "    print(\"already deleted\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rules'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spliting the data\n",
    "df_true = df.loc[df['rules']==1].copy()\n",
    "df_true.head(2)\n",
    "### total true value is 3192\n",
    "## split the data of false\n",
    "df_false = df.loc[df['rules']==0].copy()\n",
    "df_false.head(2)\n",
    "df_false['rules'].count\n",
    "### Total value of flase is 30799 prod rejected \n",
    "### Note taking randome value from each\n",
    "df_false_train = df_false.sample(4000)\n",
    "df_false_train.head(3)\n",
    "### merge both the data frame\n",
    "df_train_1 = [df_true,df_false_train]\n",
    "df_train = pd.concat(df_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()\n",
    "### Droping the column which has - until a solution is found\n",
    "df_train.columns\n",
    "### Need to reomve String value else the model will fail  or convert them using any group value\n",
    "list1 = list(df_train)\n",
    "for i in list1:\n",
    "    if df_train[i].dtype == 'object':\n",
    "        try:\n",
    "            df_train[i] = df_train[i].astype(float)\n",
    "        except:\n",
    "            print(f\"unable at columns {i}\")\n",
    "\n",
    "### conver this col to decimal \n",
    "df_train['Raw_Mat_Strength_Additive_Percent'] = df_train['Raw_Mat_Strength_Additive_Percent'].apply(lambda x: float(x.replace('%',''))/100 if isinstance(x,str) and '%' in x else x)\n",
    "df_train['Raw_Mat_Strength_Additive_Percent']\n",
    "\n",
    "### Dropping additive for now\n",
    "try:\n",
    "    df_train.drop('SKU',inplace=True,axis=1)\n",
    "    df_train.drop('Raw_Mat_Strength_Type',inplace=True,axis=1)\n",
    "    df_train.drop('Raw_Mat_Strength_Additive',inplace= True, axis=1)\n",
    "    df_train.drop('BM',inplace=True,axis=1)\n",
    "    df_train.drop('BL',inplace=True,axis=1)\n",
    "    df_train.drop('CM',inplace=True,axis=1)\n",
    "except:\n",
    "    print(\"hmm already deleted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices = np.random.permutation(len(df_train))\n",
    "test_size_data = int(len(df_train) * 0.2)\n",
    "test_indices = shuffled_indices[:test_size_data]\n",
    "train_indices = shuffled_indices[test_size_data:]\n",
    "df_train.iloc[train_indices]\n",
    "df_train.iloc[test_indices]\n",
    "print(len(train_indices)) #### train value split to 80 %\n",
    "print(len(test_indices)) ##### test value split to 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train['rules']\n",
    "features = ['BCT1', 'BCT2', 'BCT3', 'BCT4', 'BCT5', 'BCT_Avg', 'BCT_Spec', 'ECT1',\n",
    "       'ECT2', 'ECT3', 'ECT4', 'ECT5', 'ECT_Avg', 'Speed_Act', 'Feed_Roll_Gap',\n",
    "       'Printing_Cylinder_Gap_1', 'Printing_Cylinder_Gap_2',\n",
    "       'Printing_Cylinder_Gap_3', 'Printing_Cylinder_Gap_4',\n",
    "       'GLWeigth(Grammage)', 'BMWeigth(Grammage)', 'BLWeigth(Grammage)',\n",
    "       'CMWeigth(Grammage)', 'CLWeigth(Grammage)', 'Finished_Product_Wid',\n",
    "       'Finished_Product_Leg', 'Finished_Product_Hig',\n",
    "       'Raw_Mat_Strength_Additive_Percent','Speed_Act_bin']\n",
    "x = df_train[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid , y_train , y_valid = train_test_split(y,x,train_size=0.8,test_size=0.2,random_state=1)\n",
    "one_hot_code = OneHotEncoder(handle_unknown='ignore')\n",
    "numerical_imputer = SimpleImputer(strategy='mean')\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_transformer = Pipeline(steps=[\n",
    "  ('imputer', numerical_imputer),\n",
    "  ('scaler', StandardScaler())\n",
    "])\n",
    "# impute and encode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "  ('imputer', categorical_imputer),\n",
    "  ('onehot', one_hot_code)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols =  x_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = x_train.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "  transformers=[\n",
    "    ('num', numerical_transformer, numerical_cols),\n",
    "    ('cat', categorical_transformer, categorical_cols),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_estimators=9, random_state=1)\n",
    "pipeline = Pipeline(steps=[\n",
    "  ('preprocessor', preprocessor),\n",
    "  ('model', model)\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datamining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
